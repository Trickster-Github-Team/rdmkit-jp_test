## **分析（データ分析）とは？**

分析（データ分析）とは、収集したデータを調べてデータセットの傾向を判断したり、統計（またはモデル）を利用してデータ間の関係を特定したりすることです。データの種類（量的または質的）に応じて、データ分析の方法は異なります。データ分析は、データ処理（多くの場合、自動化されたバッチ処理）の後に行われます。

## **なぜデータ分析が重要なのか？**

データ分析は、新たな知識や情報が生み出される段階であるため、研究プロセスの中心であると考えることができます。データ分析は、研究成果との関連性が高いため、データセットに適用される分析ワークフローはFAIR原則に準拠していることが不可欠であり、他の研究者によって再現可能であることが非常に重要です。

## **データ分析のために何を考慮すべきか？**

データにはその内容や規模に応じて「スモールデータ」と「ビッグデータ」があります。そのため、データ分析に用いる手法や技術も異なる場合があります。「ビッグデータ」は、Volume（規模）、Velocity（速度）、Variety（多様性）、Variability（変動性）、Veracity（真実性）、Visualization（視覚化）、Value（価値）など、頭文字が「V」で始まる特性が多く関連すると言われています。

* データ分析の品質は、前の段階（収集、加工）に依存します。正確で信頼できるデータを提供することが新しい知識の基礎となります。
* 計算機環境への距離を考えると、データの置き場所は重要です。異なるインフラ間のデータ転送速度は作業効率に影響を与えます。膨大な量のデータを転送する場合にかかるコストと、分析用マシンの仮想イメージを転送する場合のコストを比較してみましょう。
* データを分析するためには、まず計算機環境を検討し、クラスタ環境やクラウド環境など、いくつかの計算機基盤の中から選択する必要があります。また、自分のニーズや専門知識に応じて、適切な作業環境（コマンドラインツール、Webブラウザ）を選択する必要があります。
* データの分析に最適なツールを選択する必要があります。
* データ分析に使用した正確な手順を文書化することが重要です。これには、使用したソフトウェアのバージョン、使用されているパラメータ、および計算機環境が含まれます。データを手動で「操作」すると、この文書化プロセスが煩雑になる場合があります。
* 共同でデータ分析を行う場合は、共同研究者がデータやツールにアクセスできるようにする必要があります。これは、クラウド等の仮想的な分析環境を設定することでも実現できます。
* データセットだけでなく、FAIR原則に従って分析ワークフローを公開することも検討してください。